# -*- coding: utf-8 -*-
"""2Praktika.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DzVOKR7n30gR-hiTPSJciGe-bUmk2_IO
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV, JSON file I/O (e.g. pd.read_csv)


from matplotlib import pyplot as plt

fake = pd.read_csv('../Fake.csv')
real = pd.read_csv('../True.csv')

print("fake luzeera")
print(len(fake))
print("real luzeera")
print(len(real))

fake['label'] = 0
real['label'] = 1

hasiera_data = real.append(fake)

labelak = hasiera_data['label']

hasiera_data

#@title Testua garbitu

import re

# Remove punctuation
hasiera_data['text_processed'] = \
hasiera_data['text'].map(lambda x: re.sub('[,\.!?()\']', '', x))

# Convert the titles to lowercase
hasiera_data['text_processed'] = \
hasiera_data['text_processed'].map(lambda x: x.lower())


import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from gensim.utils import simple_preprocess

print('testua garbitzen')

stop_words = stopwords.words('english')

def sent_to_words(sentences):
  for sentence in sentences:
  # deacc=True removes punctuations
    yield(simple_preprocess(str(sentence), deacc=True))
    
def remove_stopwords(texts):
  return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

data = hasiera_data.text_processed.values.tolist()
data_words = list(sent_to_words(data))

# remove stop words
data_words = remove_stopwords(data_words)

#@title countvectorizer erabiliko da hitzak tokenizatzeko eta TF-IDF

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

from decimal import Decimal


train_tamaina = Decimal('0.75')

emaitza=[]
ehuneko=[]

for x in range(9):
  X_train, X_test, y_train, y_test = train_test_split(hasiera_data, labelak, train_size=float(train_tamaina), test_size=0.25, random_state=3)

  text_clf = Pipeline([
                     ('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf', MultinomialNB()),
  ])
  text_clf = text_clf.fit(X_train.text_processed, y_train)

  predicted = text_clf.predict(X_test.text_processed)
  a = np.mean(predicted == y_test)
  print('accuracy:' + str(train_tamaina) + ' erabilia trainean, lortutako emaitza ' + str(a))

  ehuneko.append(float(train_tamaina))
  emaitza.append(a)

  train_tamaina = train_tamaina - Decimal('0.05 ')


data = {
    'emaitza':emaitza,
    'erabilitako ehuneko':ehuneko
}

#@title klasifikazio algoritmoak

import seaborn as sns

df = pd.DataFrame(data)


sns.lmplot(
    data=df,
    x='erabilitako ehuneko', y='emaitza',
    line_kws={'color': 'red'}
)

plt.savefig('./irudiakNB/regLin.png')